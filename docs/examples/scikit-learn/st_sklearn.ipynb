{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ec082e",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/fralfaro/DS-Cheat-Sheets/blob/main/docs/examples/scikit-learn/st_sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    " \n",
    "# Scikit-Learn Cheat Sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaaca45",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #000; background-color: #fff; color: #000; padding: 10px; display: flex; align-items: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/fralfaro/DS-Cheat-Sheets/main/docs/images/info_icon.png\" width=\"100\" style=\"margin-right: 10px;\">\n",
    "    <div>\n",
    "        <strong>Note</strong><br>\n",
    "        If you want to run this example on <a href=\"https://colab.research.google.com/\" target=\"_blank\">Google Colab</a>, follow these detailed steps below:\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "1. Install the necessary libraries:\n",
    "\n",
    "    ```python\n",
    "    !pip install streamlit\n",
    "    ```\n",
    "\n",
    "2. Create your app by executing the following cell:\n",
    "\n",
    "    ```python\n",
    "    %%writefile app.py\n",
    "    import streamlit as st\n",
    "    import pandas as pd\n",
    "    # ... (rest of your code)\n",
    "    ```\n",
    "\n",
    "3. Start your app by running this cell:\n",
    "\n",
    "    ```python\n",
    "    !streamlit run app.py & npx localtunnel --port 8501\n",
    "    ```\n",
    "\n",
    "    ![Example Image](https://raw.githubusercontent.com/fralfaro/DS-Cheat-Sheets/main/docs/images/img_01.png)\n",
    "\n",
    "    * After completing the above steps, click  \"**your url is: ...**\"  (for example, *https://major-weeks-clap.loca.lt*).  \n",
    "    * In the new  window, enter the numbers in the \"**External URL: ...**\" section (for example, **35.230.186.60**). \n",
    "    * Finally, click **Click to Submit**\n",
    "\n",
    "    ![Example Image](https://raw.githubusercontent.com/fralfaro/DS-Cheat-Sheets/main/docs/images/img_02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cadffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceb880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "from pathlib import Path\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "\n",
    "# Initial page config\n",
    "st.set_page_config(\n",
    "    page_title='Scikit-Learn Cheat Sheet',\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\",\n",
    ")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to set up the Streamlit app layout.\n",
    "    \"\"\"\n",
    "    cs_sidebar()\n",
    "    cs_body()\n",
    "    return None\n",
    "\n",
    "# Define img_to_bytes() function\n",
    "def img_to_bytes(img_url):\n",
    "    response = requests.get(img_url)\n",
    "    img_bytes = response.content\n",
    "    encoded = base64.b64encode(img_bytes).decode()\n",
    "    return encoded\n",
    "\n",
    "# Define the cs_sidebar() function\n",
    "def cs_sidebar():\n",
    "    \"\"\"\n",
    "    Populate the sidebar with various content sections related to Scikit-learn.\n",
    "    \"\"\"\n",
    "    st.sidebar.markdown(\n",
    "        '''[<img src='data:image/png;base64,{}' class='img-fluid' width=95 >](https://streamlit.io/)'''.format(\n",
    "            img_to_bytes(\"https://raw.githubusercontent.com/fralfaro/DS-Cheat-Sheets/main/docs/examples/scikit-learn/scikit-learn.png\")), unsafe_allow_html=True)\n",
    "\n",
    "    st.sidebar.header('Scikit-Learn Cheat Sheet')\n",
    "    st.sidebar.markdown('''\n",
    "<small>[Scikit-learn](https://scikit-learn.org/) is an open source Python library that\n",
    " implements a range of\n",
    "machine learning,\n",
    " preprocessing, cross-validation and visualization\n",
    "algorithms using a unified interface.</small>\n",
    "    ''', unsafe_allow_html=True)\n",
    "\n",
    "    # Scikit-Learn installation and import\n",
    "    st.sidebar.markdown('__Install and import Scikit-Learn__')\n",
    "    st.sidebar.code('$ pip install scikit-learn')\n",
    "    st.sidebar.code('''\n",
    "# Import Scikit-Learn convention\n",
    ">>> import sklearn\n",
    "''')\n",
    "\n",
    "    # Add the Scikit-learn example\n",
    "    st.sidebar.markdown('__Scikit-learn Example__')\n",
    "    st.sidebar.markdown(\n",
    "        '''[<img src='data:image/png;base64,{}' class='img-fluid' width=450 >](https://streamlit.io/)'''.format(\n",
    "            img_to_bytes(\"https://raw.githubusercontent.com/fralfaro/DS-Cheat-Sheets/main/docs/examples/scikit-learn/sk-tree.png\")), unsafe_allow_html=True)\n",
    "\n",
    "    st.sidebar.code(\"\"\"\n",
    "    from sklearn import neighbors, datasets, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X, y = iris.data[:, :2], iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=33)\n",
    "\n",
    "# Standardize the features using StandardScaler\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a K-Nearest Neighbors classifier\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target values on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\"\"\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Define the cs_body() function\n",
    "def cs_body():\n",
    "    \"\"\"\n",
    "    Create content sections for the main body of the Streamlit cheat sheet with Scikit-learn examples.\n",
    "    \"\"\"\n",
    "    col1, col2, col3 = st.columns(3)  # Create columns for layout\n",
    "\n",
    "    #######################################\n",
    "    # COLUMN 1\n",
    "    #######################################\n",
    "\n",
    "    # Loading The Data\n",
    "    col1.subheader('Loading The Data')\n",
    "    col1.code('''\n",
    "    from sklearn import datasets\n",
    "\n",
    "    # Load the Iris dataset\n",
    "    iris = datasets.load_iris()\n",
    "\n",
    "    # Split the dataset into features (X) and target (y)\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    # Print the lengths of X and y\n",
    "    print(\"Size of X:\", X.shape) #  (150, 4)\n",
    "    print(\"Size of y:\", y.shape) #  (150, )\n",
    "        ''')\n",
    "\n",
    "    # Training And Test Data\n",
    "    col1.subheader('Training And Test Data')\n",
    "    col1.code('''\n",
    "    # Import train_test_split from sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Split the data into training and test sets with test_size=0.2 (20% for test set)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "        ''')\n",
    "\n",
    "    # Create instances of the models\n",
    "    col1.subheader('Create instances of the models')\n",
    "    col1.code('''\n",
    "        # Import necessary classes from sklearn libraries\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        from sklearn.svm import SVC\n",
    "        from sklearn.cluster import KMeans\n",
    "        from sklearn.decomposition import PCA\n",
    "\n",
    "        # Create instances of supervised learning models\n",
    "        # Logistic Regression classifier (max_iter=1000)\n",
    "        lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "        # k-Nearest Neighbors classifier with 5 neighbors\n",
    "        knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "        # Support Vector Machine classifier\n",
    "        svc = SVC()\n",
    "\n",
    "        # Create instances of unsupervised learning models\n",
    "        # k-Means clustering with 3 clusters and 10 initialization attempts\n",
    "        k_means = KMeans(n_clusters=3, n_init=10)\n",
    "\n",
    "        # Principal Component Analysis with 2 components\n",
    "        pca = PCA(n_components=2)\n",
    "        ''')\n",
    "\n",
    "\n",
    "    # Model Fitting\n",
    "    col1.subheader('Model Fitting')\n",
    "    col1.code('''\n",
    "    # Supervised learning\n",
    "    lr.fit(X_train, y_train)\n",
    "    knn.fit(X_train, y_train)\n",
    "    svc.fit(X_train, y_train)\n",
    "\n",
    "    # Unsupervised Learning\n",
    "    k_means.fit(X_train)\n",
    "    pca.fit_transform(X_train)\n",
    "        ''')\n",
    "\n",
    "    # Prediction\n",
    "    col1.subheader('Prediction')\n",
    "    col1.code('''\n",
    "    # Supervised Estimators\n",
    "    y_pred = svc.predict(X_test) # Predict labels\n",
    "    y_pred = lr.predict(X_test) # Predict labels\n",
    "    y_pred = knn.predict_proba(X_test) # Estimate probability of a label\n",
    "\n",
    "    # Unsupervised Estimators\n",
    "    y_pred = k_means.predict(X_test) # Predict labels in clustering algos\n",
    "        ''')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #######################################\n",
    "    # COLUMN 2\n",
    "    #######################################\n",
    "\n",
    "    # Preprocessing The Data\n",
    "\n",
    "    # Standardization\n",
    "    col2.subheader('Standardization')\n",
    "    col2.code('''\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Create an instance of the StandardScaler and fit it to training data\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "    # Transform the training and test data using the scaler\n",
    "    standardized_X = scaler.transform(X_train)\n",
    "    standardized_X_test = scaler.transform(X_test)\n",
    "    ''')\n",
    "\n",
    "    # Normalization\n",
    "    col2.subheader('Normalization')\n",
    "    col2.code('''\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    scaler = Normalizer().fit(X_train)\n",
    "    normalized_X = scaler.transform(X_train)\n",
    "    normalized_X_test = scaler.transform(X_test)\n",
    "    ''')\n",
    "\n",
    "    # Binarization\n",
    "    col2.subheader('Binarization')\n",
    "    col2.code('''\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import Binarizer\n",
    "\n",
    "    # Create a sample data array\n",
    "    data = np.array([[1.5, 2.7, 0.8],\n",
    "                     [0.2, 3.9, 1.2],\n",
    "                     [4.1, 1.0, 2.5]])\n",
    "\n",
    "    # Create a Binarizer instance with a threshold of 2.0\n",
    "    binarizer = Binarizer(threshold=2.0)\n",
    "\n",
    "    # Apply binarization to the data\n",
    "    binarized_data = binarizer.transform(data)\n",
    "    ''')\n",
    "\n",
    "    # Encoding Categorical Features\n",
    "    col2.subheader('Encoding Categorical Features')\n",
    "    col2.code('''\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    # Sample data: categorical labels\n",
    "    labels = ['cat', 'dog', 'dog', 'fish', 'cat', 'dog', 'fish']\n",
    "\n",
    "    # Create a LabelEncoder instance\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Fit and transform the labels\n",
    "    encoded_labels = label_encoder.fit_transform(labels)\n",
    "    ''')\n",
    "\n",
    "    # Imputing Missing Values\n",
    "    col2.subheader('Imputing Missing Values')\n",
    "    col2.code('''\n",
    "    import numpy as np\n",
    "    from sklearn.impute import SimpleImputer\n",
    "\n",
    "    # Sample data with missing values\n",
    "    data = np.array([[1.0, 2.0, np.nan],\n",
    "                     [4.0, np.nan, 6.0],\n",
    "                     [7.0, 8.0, 9.0]])\n",
    "\n",
    "    # Create a SimpleImputer instance with strategy='mean'\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "    # Fit and transform the imputer on the data\n",
    "    imputed_data = imputer.fit_transform(data)\n",
    "    ''')\n",
    "\n",
    "    # Generating Polynomial Features\n",
    "    col2.subheader('Generating Polynomial Features')\n",
    "    col2.code('''\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "    # Sample data\n",
    "    data = np.array([[1, 2],\n",
    "                     [3, 4],\n",
    "                     [5, 6]])\n",
    "\n",
    "    # Create a PolynomialFeatures instance of degree 2\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "    # Transform the data to include polynomial features\n",
    "    poly_data = poly.fit_transform(data)\n",
    "    ''')\n",
    "\n",
    "\n",
    "\n",
    "    #######################################\n",
    "    # COLUMN 3\n",
    "    #######################################\n",
    "\n",
    "    # Comparison operations\n",
    "    # Classification Metrics\n",
    "    col3.subheader('Classification Metrics')\n",
    "    col3.code('''\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "    # Accuracy Score\n",
    "    accuracy_knn = knn.score(X_test, y_test)\n",
    "    print(\"Accuracy Score (knn):\", knn.score(X_test, y_test))\n",
    "\n",
    "    accuracy_y_pred = accuracy_score(y_test, y_pred_lr)\n",
    "    print(\"Accuracy Score (y_pred):\", accuracy_y_pred)\n",
    "\n",
    "    # Classification Report\n",
    "    classification_rep_y_pred = classification_report(y_test, y_pred_lr)\n",
    "    print(\"Classification Report (y_pred):\", classification_rep_y_pred)\n",
    "\n",
    "    classification_rep_y_pred_lr = classification_report(y_test, y_pred_lr)\n",
    "    print(\"Classification Report (y_pred_lr):\", classification_rep_y_pred_lr)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix_y_pred_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "    print(\"Confusion Matrix (y_pred_lr):\", conf_matrix_y_pred_lr)\n",
    "        ''')\n",
    "\n",
    "    # Regression Metrics\n",
    "    col3.subheader('Regression Metrics')\n",
    "    col3.code('''\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "    # Data: True/Predicted values \n",
    "    y_true = [3, -0.5, 2]\n",
    "    y_pred = [2.8, -0.3, 1.8]\n",
    "\n",
    "    # Calculate Mean Absolute Error\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "    # Calculate Mean Squared Error\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "    # Calculate R² Score\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(\"R² Score:\", r2)\n",
    "        ''')\n",
    "\n",
    "    # Clustering Metrics\n",
    "    col3.subheader('Clustering Metrics')\n",
    "    col3.code('''\n",
    "    from sklearn.metrics import adjusted_rand_score, homogeneity_score, v_measure_score\n",
    "\n",
    "    # Adjusted Rand Index\n",
    "    adjusted_rand_index = adjusted_rand_score(y_test, y_pred_kmeans)\n",
    "    print(\"Adjusted Rand Index:\", adjusted_rand_index)\n",
    "\n",
    "    # Homogeneity Score\n",
    "    homogeneity = homogeneity_score(y_test, y_pred_kmeans)\n",
    "    print(\"Homogeneity Score:\", homogeneity)\n",
    "\n",
    "    # V-Measure Score\n",
    "    v_measure = v_measure_score(y_test, y_pred_kmeans)\n",
    "    print(\"V-Measure Score:\", v_measure)\n",
    "        ''')\n",
    "\n",
    "    # Cross-Validation\n",
    "    col3.subheader('Cross-Validation')\n",
    "    col3.code('''\n",
    "    # Import necessary library\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    # Cross-validation with KNN estimator\n",
    "    knn_scores = cross_val_score(knn, X_train, y_train, cv=4)\n",
    "    print(knn_scores)\n",
    "\n",
    "    # Cross-validation with Linear Regression estimator\n",
    "    lr_scores = cross_val_score(lr, X, y, cv=2)\n",
    "    print(lr_scores)\n",
    "        ''')\n",
    "\n",
    "    # Grid Search\n",
    "    col3.subheader('Grid Search')\n",
    "    col3.code('''\n",
    "    # Import necessary library\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # Define parameter grid\n",
    "    params = {\n",
    "        'n_neighbors': np.arange(1, 3),\n",
    "        'weights': ['uniform', 'distance']\n",
    "    }\n",
    "\n",
    "    # Create GridSearchCV object\n",
    "    grid = GridSearchCV(estimator=knn, param_grid=params)\n",
    "\n",
    "    # Fit the grid to the data\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best parameters found\n",
    "    print(\"Best parameters:\", grid.best_params_)\n",
    "\n",
    "    # Print the best cross-validation score\n",
    "    print(\"Best cross-validation score:\", grid.best_score_)\n",
    "\n",
    "    # Print the accuracy on the test set using the best parameters\n",
    "    best_knn = grid.best_estimator_\n",
    "    test_accuracy = best_knn.score(X_test, y_test)\n",
    "    print(\"Test set accuracy:\", test_accuracy)\n",
    "        ''')\n",
    "\n",
    "    # Asking for Help\n",
    "    col1.subheader('Asking for Help')\n",
    "    col1.code('''\n",
    "    import sklearn.cluster\n",
    "\n",
    "    # Use the help() function to get information about the KMeans class\n",
    "    help(sklearn.cluster.KMeans)\n",
    "    ''')\n",
    "\n",
    "\n",
    "\n",
    "# Run the main function if the script is executed directly\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d966598",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run app.py & npx localtunnel --port 8501"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
