{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac393afc",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center;\">\n",
    "    <a href=\"https://github.com/fralfaro/DS-Cheat-Sheets/blob/main/docs/files/scikit-learn_cs.pdf\" target=\"_parent\" style=\"margin-right: 10px;\">\n",
    "        <img src=\"https://img.shields.io/badge/Open in PDF-%23FF0000.svg?style=flat-square&logo=adobe&logoColor=white\"/>\n",
    "    </a>\n",
    "    <a href=\"https://ds-cheat-sheets-sklearn.streamlit.app/\" target=\"_parent\" style=\"margin-right: 10px;\">\n",
    "        <img src=\"https://static.streamlit.io/badges/streamlit_badge_black_white.svg\"/>\n",
    "    </a>\n",
    "    <a href=\"https://colab.research.google.com/github/fralfaro/DS-Cheat-Sheets/blob/main/docs/examples/scikit-learn/sklearn.ipynb\" target=\"_parent\">\n",
    "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df1e26",
   "metadata": {},
   "source": [
    "# Scikit-Learn \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fralfaro/DS-Cheat-Sheets/main/docs/examples/scikit-learn/scikit-learn.png\" alt=\"numpy logo\" width = \"200\">\n",
    "\n",
    "[Scikit-learn](https://scikit-learn.org/) is an open source Python library that\n",
    " implements a range of\n",
    "machine learning,\n",
    " preprocessing, cross-validation and visualization\n",
    "algorithms using a unified interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e2ffce",
   "metadata": {},
   "source": [
    "## Install and import Scikit-Learn\n",
    "\n",
    "`\n",
    "$ pip install scikit-learn\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a0de1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Scikit-Learn convention\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d13116a",
   "metadata": {},
   "source": [
    "## Scikit-learn Example\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fralfaro/DS-Cheat-Sheets/main/docs/examples/scikit-learn/sk-tree.png\" alt=\"numpy logo\" width = \"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb151a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.631578947368421\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors, datasets, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X, y = iris.data[:, :2], iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=33)\n",
    "\n",
    "# Standardize the features using StandardScaler\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a K-Nearest Neighbors classifier\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target values on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1844fe7b",
   "metadata": {},
   "source": [
    "## Loading The Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c736518f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: (150, 4)\n",
      "Size of y: (150,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Print the lengths of X and y\n",
    "print(\"Size of X:\", X.shape) #  (150, 4)\n",
    "print(\"Size of y:\", y.shape) #  (150, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b288da8f",
   "metadata": {},
   "source": [
    "## Training And Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b47452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_train: (120, 4)\n",
      "Size of X_test:  (30, 4)\n",
      "Size of y_train: (120,)\n",
      "Size of y_test:  (30,)\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets with test_size=0.2 (20% for test set)\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Print the sizes of the arrays\n",
    "print(\"Size of X_train:\", X_train.shape)\n",
    "print(\"Size of X_test: \", X_test.shape)\n",
    "print(\"Size of y_train:\", y_train.shape)\n",
    "print(\"Size of y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e21fc",
   "metadata": {},
   "source": [
    "## Create instances of the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8559bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary classes from sklearn libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create instances of supervised learning models\n",
    "# Logistic Regression classifier (max_iter=1000)\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# k-Nearest Neighbors classifier with 5 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Support Vector Machine classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Create instances of unsupervised learning models\n",
    "# k-Means clustering with 3 clusters and 10 initialization attempts\n",
    "k_means = KMeans(n_clusters=3, n_init=10)\n",
    "\n",
    "# Principal Component Analysis with 2 components\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8995fc3",
   "metadata": {},
   "source": [
    "## Model Fitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f00a391f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: LogisticRegression(max_iter=1000)\n",
      "knn: KNeighborsClassifier()\n",
      "svc: SVC()\n",
      "k_means: KMeans(n_clusters=3, n_init=10)\n",
      "pca: PCA(n_components=2)\n"
     ]
    }
   ],
   "source": [
    "# Fit models to the data\n",
    "lr.fit(X_train, y_train)\n",
    "knn.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "k_means.fit(X_train)\n",
    "pca.fit_transform(X_train)\n",
    "\n",
    "# Print the instances and models\n",
    "print(\"lr:\", lr)\n",
    "print(\"knn:\", knn)\n",
    "print(\"svc:\", svc)\n",
    "print(\"k_means:\", k_means)\n",
    "print(\"pca:\", pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a7bb0",
   "metadata": {},
   "source": [
    "## Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3954fe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised Estimators:\n",
      "SVC predictions: [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0]\n",
      "Logistic Regression predictions: [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0]\n",
      "KNeighborsClassifier probabilities:\n",
      " [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]] \n",
      "     ...\n",
      "\n",
      "Unsupervised Estimators:\n",
      "KMeans predictions: [2 2 0 1 0 1 0 2 2 2 1 2 2 2 2 0 2 2 0 0 2 2 0 0 2 0 0 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "# Predict using different supervised estimators\n",
    "y_pred_svc = svc.predict(X_test)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "y_pred_knn_proba = knn.predict_proba(X_test)\n",
    "\n",
    "\n",
    "# Predict labels using KMeans in clustering algorithms\n",
    "y_pred_kmeans = k_means.predict(X_test)\n",
    "\n",
    "# Print the results\n",
    "print(\"Supervised Estimators:\")\n",
    "print(\"SVC predictions:\", y_pred_svc)\n",
    "print(\"Logistic Regression predictions:\", y_pred_lr)\n",
    "print(\"KNeighborsClassifier probabilities:\\n\", y_pred_knn_proba[:5],\"\\n     ...\")\n",
    "\n",
    "print(\"\\nUnsupervised Estimators:\")\n",
    "print(\"KMeans predictions:\", y_pred_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea556d7",
   "metadata": {},
   "source": [
    "## Preprocessing The Data\n",
    "\n",
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d262ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardized X_train:\n",
      " [[ 0.61303014  0.10850105  0.94751783  0.736072  ]\n",
      " [-0.56776627 -0.12400121  0.38491447  0.34752959]\n",
      " [-0.80392556  1.03851009 -1.30289562 -1.33615415]\n",
      " [ 0.25879121 -0.12400121  0.60995581  0.736072  ]\n",
      " [ 0.61303014 -0.58900572  1.00377816  1.25412853]] \n",
      "     ...\n",
      "\n",
      "Standardized X_test:\n",
      " [[-0.09544771 -0.58900572  0.72247648  1.5131568 ]\n",
      " [ 0.14071157 -1.98401928  0.10361279 -0.30004108]\n",
      " [-0.44968663  2.66602591 -1.35915595 -1.33615415]\n",
      " [ 1.6757469  -0.35650346  1.39760052  0.736072  ]\n",
      " [-1.04008484  0.80600783 -1.30289562 -1.33615415]] \n",
      "     ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create an instance of the StandardScaler and fit it to training data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Transform the training and test data using the scaler\n",
    "standardized_X = scaler.transform(X_train)\n",
    "standardized_X_test = scaler.transform(X_test)\n",
    "\n",
    "# Print the variables\n",
    "print(\"\\nStandardized X_train:\\n\", standardized_X[:5],\"\\n     ...\")\n",
    "print(\"\\nStandardized X_test:\\n\", standardized_X_test[:5],\"\\n     ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58eed58",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32614426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalized X_train:\n",
      " [[0.69804799 0.338117   0.59988499 0.196326  ]\n",
      " [0.69333409 0.38518561 0.57777841 0.1925928 ]\n",
      " [0.80641965 0.54278246 0.23262105 0.03101614]\n",
      " [0.71171214 0.35002236 0.57170319 0.21001342]\n",
      " [0.69417747 0.30370264 0.60740528 0.2386235 ]] \n",
      "     ...\n",
      "\n",
      "Normalized X_test:\n",
      " [[0.67767924 0.32715549 0.59589036 0.28041899]\n",
      " [0.78892752 0.28927343 0.52595168 0.13148792]\n",
      " [0.77867447 0.59462414 0.19820805 0.02831544]\n",
      " [0.71366557 0.28351098 0.61590317 0.17597233]\n",
      " [0.80218492 0.54548574 0.24065548 0.0320874 ]] \n",
      "     ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer().fit(X_train)\n",
    "normalized_X = scaler.transform(X_train)\n",
    "normalized_X_test = scaler.transform(X_test)\n",
    "\n",
    "# Print the variables\n",
    "print(\"\\nNormalized X_train:\\n\", normalized_X[:5],\"\\n     ...\")\n",
    "print(\"\\nNormalized X_test:\\n\", normalized_X_test[:5],\"\\n     ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9209670",
   "metadata": {},
   "source": [
    "### Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a6367d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "[[1.5 2.7 0.8]\n",
      " [0.2 3.9 1.2]\n",
      " [4.1 1.  2.5]]\n",
      "\n",
      "Binarized data:\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Create a sample data array\n",
    "data = np.array([[1.5, 2.7, 0.8],\n",
    "                 [0.2, 3.9, 1.2],\n",
    "                 [4.1, 1.0, 2.5]])\n",
    "\n",
    "# Create a Binarizer instance with a threshold of 2.0\n",
    "binarizer = Binarizer(threshold=2.0)\n",
    "\n",
    "# Apply binarization to the data\n",
    "binarized_data = binarizer.transform(data)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nBinarized data:\")\n",
    "print(binarized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4196cfd",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1233524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels: ['cat', 'dog', 'dog', 'fish', 'cat', 'dog', 'fish']\n",
      "Encoded labels: [0 1 1 2 0 1 2]\n",
      "Decoded labels: ['cat' 'dog' 'dog' 'fish' 'cat' 'dog' 'fish']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample data: categorical labels\n",
    "labels = ['cat', 'dog', 'dog', 'fish', 'cat', 'dog', 'fish']\n",
    "\n",
    "# Create a LabelEncoder instance\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Print the original labels and their encoded versions\n",
    "print(\"Original labels:\", labels)\n",
    "print(\"Encoded labels:\", encoded_labels)\n",
    "\n",
    "# Decode the encoded labels back to the original labels\n",
    "decoded_labels = label_encoder.inverse_transform(encoded_labels)\n",
    "print(\"Decoded labels:\", decoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44306b3f",
   "metadata": {},
   "source": [
    "### Imputing Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efa74719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "[[ 1.  2. nan]\n",
      " [ 4. nan  6.]\n",
      " [ 7.  8.  9.]]\n",
      "\n",
      "Imputed data:\n",
      "[[1.  2.  7.5]\n",
      " [4.  5.  6. ]\n",
      " [7.  8.  9. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Sample data with missing values\n",
    "data = np.array([[1.0, 2.0, np.nan],\n",
    "                 [4.0, np.nan, 6.0],\n",
    "                 [7.0, 8.0, 9.0]])\n",
    "\n",
    "# Create a SimpleImputer instance with strategy='mean'\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit and transform the imputer on the data\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nImputed data:\")\n",
    "print(imputed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0374a",
   "metadata": {},
   "source": [
    "### Generating Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55404919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "\n",
      "Polynomial features:\n",
      "[[ 1.  1.  2.  1.  2.  4.]\n",
      " [ 1.  3.  4.  9. 12. 16.]\n",
      " [ 1.  5.  6. 25. 30. 36.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 2],\n",
    "                 [3, 4],\n",
    "                 [5, 6]])\n",
    "\n",
    "# Create a PolynomialFeatures instance of degree 2\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "# Transform the data to include polynomial features\n",
    "poly_data = poly.fit_transform(data)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nPolynomial features:\")\n",
    "print(poly_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ee4f6c",
   "metadata": {},
   "source": [
    "## Classification Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcd4d4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score (knn): 0.9666666666666667\n",
      "Accuracy Score (y_pred): 1.0\n",
      "Classification Report (y_pred):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Classification Report (y_pred_lr):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Confusion Matrix (y_pred_lr):\n",
      " [[11  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0  6]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Accuracy Score\n",
    "accuracy_knn = knn.score(X_test, y_test)\n",
    "print(\"Accuracy Score (knn):\", knn.score(X_test, y_test))\n",
    "\n",
    "accuracy_y_pred = accuracy_score(y_test, y_pred_lr)\n",
    "print(\"Accuracy Score (y_pred):\", accuracy_y_pred)\n",
    "\n",
    "# Classification Report\n",
    "classification_rep_y_pred = classification_report(y_test, y_pred_lr)\n",
    "print(\"Classification Report (y_pred):\\n\", classification_rep_y_pred)\n",
    "\n",
    "classification_rep_y_pred_lr = classification_report(y_test, y_pred_lr)\n",
    "print(\"Classification Report (y_pred_lr):\\n\", classification_rep_y_pred_lr)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_y_pred_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "print(\"Confusion Matrix (y_pred_lr):\\n\", conf_matrix_y_pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d172e271",
   "metadata": {},
   "source": [
    "## Regression Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c952c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.20000000000000004\n",
      "Mean Squared Error: 0.040000000000000015\n",
      "R² Score: 0.9815384615384616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# True values (ground truth)\n",
    "y_true = [3, -0.5, 2]\n",
    "\n",
    "# Predicted values\n",
    "y_pred = [2.8, -0.3, 1.8]\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate R² Score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(\"R² Score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca9d3c1",
   "metadata": {},
   "source": [
    "## Clustering Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "162f3aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index: 0.7657144139494176\n",
      "Homogeneity Score: 0.7553796021571243\n",
      "V-Measure Score: 0.8005552543570766\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, homogeneity_score, v_measure_score\n",
    "\n",
    "# Adjusted Rand Index\n",
    "adjusted_rand_index = adjusted_rand_score(y_test, y_pred_kmeans)\n",
    "print(\"Adjusted Rand Index:\", adjusted_rand_index)\n",
    "\n",
    "# Homogeneity Score\n",
    "homogeneity = homogeneity_score(y_test, y_pred_kmeans)\n",
    "print(\"Homogeneity Score:\", homogeneity)\n",
    "\n",
    "# V-Measure Score\n",
    "v_measure = v_measure_score(y_test, y_pred_kmeans)\n",
    "print(\"V-Measure Score:\", v_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d91e042",
   "metadata": {},
   "source": [
    "## Cross-Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73d8f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96666667 0.93333333 1.         0.93333333]\n",
      "[0.96 0.96]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary library\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Cross-validation with KNN estimator\n",
    "knn_scores = cross_val_score(knn, X_train, y_train, cv=4)\n",
    "print(knn_scores)\n",
    "\n",
    "# Cross-validation with Linear Regression estimator\n",
    "lr_scores = cross_val_score(lr, X, y, cv=2)\n",
    "print(lr_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b948b913",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8ee5a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_neighbors': 1, 'weights': 'uniform'}\n",
      "Best cross-validation score: 0.9416666666666667\n",
      "Test set accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary library\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "params = {\n",
    "    'n_neighbors': np.arange(1, 3),\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid = GridSearchCV(estimator=knn, param_grid=params)\n",
    "\n",
    "# Fit the grid to the data\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "\n",
    "# Print the best cross-validation score\n",
    "print(\"Best cross-validation score:\", grid.best_score_)\n",
    "\n",
    "# Print the accuracy on the test set using the best parameters\n",
    "best_knn = grid.best_estimator_\n",
    "test_accuracy = best_knn.score(X_test, y_test)\n",
    "print(\"Test set accuracy:\", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
